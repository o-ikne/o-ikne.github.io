<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Publications - My Academic Web Page</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <h1>Publications</h1>
    </header>
    <nav>
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="research.html">Research</a></li>
            <li><a href="publications.html">Publications</a></li>
            <li><a href="cv.html">CV</a></li>
            <li><a href="contact.html">Contact</a></li>
        </ul>
    </nav>
    <main>
        <section id="publications">
            <div class="publication-section">
                <h2>Conference Papers</h2>
                <div class="publication">
                    <h3>Skeleton-based Self-Supervised Feature Extraction for Improved Dynamic Hand Gesture Recognition</h3>
                    <p><strong>Authors:</strong> Omar Ikne, Benjamin Allaert, Hazem Wannous</p>
                    <p><strong>Conference:</strong> The 18th IEEE International Conference on Automatic Face and Gesture Recognition, 2024</p>
                    <a href="link-to-paper-1" class="paper-link">Read Paper</a>
                    <button onclick="toggleAbstract('abstract1')">Show Abstract</button>
                    <p id="abstract1" class="abstract">Human-computer interaction has become an essential
                        part of our lives, especially with the particularly with the rise of digital environments.
                        Nevertheless, the challenge persists through hand gestures, given the complexities associated
                        with factors like pose variation and occlusions. In this paper, we propose an innovative approach
                        to improve skeleton-based hand gesture recognition by integrating self-supervised learning, a 
                        promising technique for acquiring distinctive representations directly from unlabeled data.
                        The proposed method takes advantage of prior knowledge of hand topology, combining 
                        topology-aware self-supervised learning with a customized skeleton-based architecture
                        to derive meaningful representations from skeleton data under different hand poses.
                        We introduce customized masking strategies for skeletal hand data and design a model
                        architecture that incorporates spatial connectivity information, improving the model’s 
                        understanding of the interrelationships between hand joints. The extensive experiments
                        demonstrate the effectiveness of the approach, with state-of-the-art performance on benchmark 
                        datasets. An exploration of the generalization of learned representations across datasets and a 
                        study of the impact of fine-tuning with limited labeled data are conducted, 
                        highlighting the adaptability and robustness of the proposed approach.</p>
                </div>
                <div class="publication">
                    <h3>Spatio-Temporal Sparse Graph Convolution Network for Hand Gesture Recognition</h3>
                    <p><strong>Authors:</strong> Omar Ikne, Rim Slama, Hichem Saoudi, Hazem Wannous</p>
                    <p><strong>Conference:</strong>The 18th IEEE International Conference on Automatic Face and Gesture Recognition, 2024</p>
                    <a href="link-to-paper-2" class="paper-link">Read Paper</a>
                    <button onclick="toggleAbstract('abstract2')">Show Abstract</button>
                    <p id="abstract2" class="abstract">Unlike whole-body action recognition, hand gestures involve spatially closely distributed joints,
                        promoting stronger collaboration. This needs to be taken into account in order to capture complex spatial and temporal features. 
                        In response to these challenges, this paper presents a Spatio-Temporal Sparse Graph Convolution Network (ST-SGCN) for dynamic 
                        recognition of hand gestures. Based on decoupled spatio-temporal processing, the ST-SGCN incorporates Graph Convolutional Networks,
                        attention mechanism and asymmetric convolutions to capture the nuanced movements of hand joints. The key novelty is the introduction
                        of sparse spatio-temporal directed interactions, overcoming the limitations associated with dense, undirected methods. 
                        The sparse aspect models essential interactions between hand joints selectively, improving computational efficiency and interpretability. 
                        Directed interactions capture asymmetrical dependencies between hand joints, improving discernment of joint influences. 
                        Experimental evaluations on three benchmark datasets, including Briareo, SHREC'17 and IPN Hand, demonstrate ST-SGCN's state-of-the-art 
                        performance for dynamic hand gesture recognition. Codes are available at: https://github.com/HichemSaoudi/ST-SGCN</p>
                </div>
                
                <div class="publication">
                    <h3>Automatic Modeling of Dynamical Interactions Within Marine Ecosystems</h3>
                    <p><strong>Authors:</strong> Omar Ikne, Maxime Folschette, Tony Ribeiro</p>
                    <p><strong>Conference:</strong>The 1st International Joint Conference on Learning & Reasoning, 2021</p>
                    <a href="https://hal.science/hal-03347033/" class="paper-link">Read Paper</a>
                    <button onclick="toggleAbstract('abstract3')">Show Abstract</button>
                    <p id="abstract3" class="abstract">Marine ecology models are used to study and anticipate population variations of plankton and
                        microalgae species. These variations can have an impact on ecological niches, the economy or the climate. Our objective is the
                        automation of the creation of such models. Learning From Interpretation Transition (LFIT) is a framework that aims at learning
                        the dynamics of a system by observing its state transitions. LFIT provides explainable predictions in the form of logical rules.
                        In this paper, we introduce a method that allows to extract an influence graph from a LFIT model. We also propose an heuristic
                        to improve the model against noise in the data.</p>
                </div>
                
                <!-- Add more conference papers as needed -->
            </div>
            <div class="publication-section">
                <h2>Journal Papers</h2>
                <div class="publication">
                    <h3>eMotion-GAN: A Motion-based GAN for Photorealistic and Facial Expression Preserving Frontal View Synthesis</h3>
                    <p><strong>Authors:</strong> Omar Ikne, Benjamin Allaert, Ioan Marius Bilasco, Hazem Wannous</p>
                    <p><strong>Journal:</strong> arXiv preprint arXiv:2404.09940, 2024</p>
                    <a href="https://arxiv.org/abs/2404.09940" class="paper-link">Read Paper</a>
                    <button onclick="toggleAbstract('abstract4')">Show Abstract</button>
                    <p id="abstract4" class="abstract">Many existing facial expression recognition (FER) systems encounter substantial performance
                        degradation when faced with variations in head pose. Numerous frontalization methods have been proposed to enhance these systems' 
                        performance under such conditions. However, they often introduce undesirable deformations, rendering them less suitable for precise
                        facial expression analysis. In this paper, we present eMotion-GAN, a novel deep learning approach designed for frontal view synthesis
                        while preserving facial expressions within the motion domain. Considering the motion induced by head variation as noise and the motion
                        induced by facial expression as the relevant information, our model is trained to filter out the noisy motion in order to retain only 
                        the motion related to facial expression. The filtered motion is then mapped onto a neutral frontal face to generate the corresponding
                        expressive frontal face. We conducted extensive evaluations using several widely recognized dynamic FER datasets, which encompass sequences
                        exhibiting various degrees of head pose variations in both intensity and orientation. Our results demonstrate the effectiveness of our 
                        approach in significantly reducing the FER performance gap between frontal and non-frontal faces. Specifically, we achieved a FER improvement 
                        of up to +5% for small pose variations and up to +20% improvement for larger pose variations.</p>
                </div>
                <!-- Add more journal papers as needed -->
            </div>
        </section>
    </main>
    <footer>
        <p>&copy; 2024 Omar Ikne | ⵡⵉⵏ ⴰⵢⵜ ⵢⵉⴽⵏ</p>
    </footer>
    <script>
        function toggleAbstract(id) {
            var abstract = document.getElementById(id);
            if (abstract.style.display === "none" || abstract.style.display === "") {
                abstract.style.display = "block";
            } else {
                abstract.style.display = "none";
            }
        }
    </script>
</body>
</html>

